- title: "Tencentâ€™s Multilingual Machine Translation System for WMT22 Large-Scale African Languages"
  image: cover_wmt22.jpeg
  description: We adopt data augmentation, distributionally robust optimization, and language family grouping, respectively, to develop our multilingual neural machine translation (MNMT) models for African languages. 
  authors: Wenxiang Jiao, Zhaopeng Tu, Jiarui Li, Wenxuan Wang, Jen-tse Huang and Shuming Shi
  link:
    url: https://arxiv.org/abs/2210.09644
    display: WMT 2022 / 1st Place in the Competition 
  codelink:
    url: https://github.com/wxjiao/WMT2022-Large-Scale-African
    display: Code
  highlight: 1
  
- title: "Adapters for Enhanced Modeling of Multilingual Knowledge and Text"
  image: adapter.png
  description: We enhance multilingual LMs with knowledge from multilingual knowledge graphs to tackle language and knowledge graph tasks across many languages.
  authors: Yifan Hou, Wenxiang Jiao, Meizhen Liu, Carl Allen, Zhaopeng Tu and Mrinmaya Sachan
  link:
    url: https://arxiv.org/abs/2210.13617
    display: EMNLP 2022 (Findings) / Best Paper Award at MRL Workshop 
  codelink:
    url: https://github.com/yifan-h/Multilingual_Space
    display: Code
  highlight: 1

- title: "Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation"
  image: cover_acl22_under.jpeg
  description: We point out the discrepancies between Seq2Seq pretraining and NMT finetuning that limit the translation quality (i.e., domain discrepancy) and induce the over-estimation issue (i.e., objective discrepancy), and propose in-domain pretraining and input adaptation as solutions.
  authors: Wenxuan Wang, Wenxiang Jiao, Yongchang Hao, Xing Wang, Shuming Shi, Zhaopeng Tu and Michael R. Lyu
  link:
    url: https://aclanthology.org/2022.acl-long.185
    display: ACL 2022
  highlight: 1
