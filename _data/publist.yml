- title: "Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation"
  image: xxx.jpeg
  description: We propose a Prompt based domain text Generation (PGEN) approach to produce the large-scale in-domain spoken language text data so as to scale back-translation for sign language translation.
  authors: Jinhui Ye*, Wenxiang Jiao*, Xing Wang and Zhaopeng Tu
  link:
    url: https://arxiv.org/pdf/2210.07054.pdf
    display: EACL 2023
  highlight: 0

- title: "Is ChatGPT A Good Translator? A Preliminary Study"
  image: cover_tech_chatgpt.png
  description: We find that ChatGPT performs competitively with commercial translation products (e.g., Google Translate) on high-resource European languages and is potentially a good translator for spoken language.
  authors: Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang and Zhaopeng Tu
  link:
    url: https://wxjiao.github.io/downloads/tech_chatgpt_arxiv.pdf
    display: Technical Report
  codelink:
    url: https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator
    display: Code
  highlight: 1

- title: "Tencentâ€™s Multilingual Machine Translation System for WMT22 Large-Scale African Languages"
  image: cover_wmt22.jpeg
  description: We adopt data augmentation, distributionally robust optimization, and language family grouping, respectively, to develop our multilingual neural machine translation (MNMT) models for African languages. 
  authors: Wenxiang Jiao, Zhaopeng Tu, Jiarui Li, Wenxuan Wang, Jen-tse Huang and Shuming Shi
  link:
    url: https://arxiv.org/abs/2210.09644
    display: WMT 2022 / 1st Place in the Competition 
  codelink:
    url: https://github.com/wxjiao/WMT2022-Large-Scale-African
    display: Code
  highlight: 1
  
- title: "Adapters for Enhanced Modeling of Multilingual Knowledge and Text"
  image: adapter.png
  description: We enhance multilingual LMs with knowledge from multilingual knowledge graphs to tackle language and knowledge graph tasks across many languages.
  authors: Yifan Hou, Wenxiang Jiao, Meizhen Liu, Carl Allen, Zhaopeng Tu and Mrinmaya Sachan
  link:
    url: https://arxiv.org/abs/2210.13617
    display: EMNLP 2022 (Findings) / Best Paper Award at MRL Workshop 
  codelink:
    url: https://github.com/yifan-h/Multilingual_Space
    display: Code
  highlight: 1

- title: "Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation"
  image: cover_acl22_under.jpeg
  description: We point out the discrepancies between Seq2Seq pretraining and NMT finetuning that limit the translation quality (i.e., domain discrepancy) and induce the over-estimation issue (i.e., objective discrepancy), and propose in-domain pretraining and input adaptation as solutions.
  authors: Wenxuan Wang, Wenxiang Jiao, Yongchang Hao, Xing Wang, Shuming Shi, Zhaopeng Tu and Michael R. Lyu
  link:
    url: https://aclanthology.org/2022.acl-long.185
    display: ACL 2022
  highlight: 1

- title: "Exploiting Inactive Examples for Natural Language Generation with Data Rejuvenation"
  image: cover_taslp22_exploit.jpeg
  description: An extended version of data rejuvenation for general NLG tasks, including both machine translation and text summarization.
  authors: Wenxiang Jiao, Xing Wang, Shilin He, Zhaopeng Tu, Irwin King and Michael R. Lyu
  link:
    url: https://ieeexplore.ieee.org/document/9721161
    display: IEEE/ACM TASLP 2022
  highlight: 1
  
- title: "Self-training Sampling with Monolingual Data Uncertainty for Neural Machine Translation"
  image: cover_acl21_selftrain.jpeg
  description: We design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability.
  authors: Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Shuming Shi, Michael R. Lyu and Irwin King
  link:
    url: https://aclanthology.org/2021.acl-long.221.pdf
    display: ACL 2021
  codelink:
    url: https://github.com/wxjiao/UncSamp
    display: Code
  highlight: 1

- title: "Data Rejuvenation: Exploiting Inactive Training Examples for Neural Machine Translation"
  image: cover_emnlp20_datareju.jpeg
  description: We introduce data rejuvenation to improve the training of NMT models on large-scale datasets by identifying and re-labeling inactive examples.
  authors: Wenxiang Jiao, Xing Wang, Shilin He, Irwin King, Michael R. Lyu and Zhaopeng Tu
  link:
    url: https://www.aclweb.org/anthology/2020.emnlp-main.176
    display: EMNLP 2020
  codelink:
    url: https://github.com/wxjiao/Data-Rejuvenation
    display: Code
  highlight: 1
  
- title: "Exploiting Unsupervised Data for Emotion Recognition in Conversations"
  image: xxx.jpeg
  description: We Pre-train a COntext-Dependent Encoder (Pre-CODE) on the conversation completion task with unlabled data to boost the performance of conversational emotion recognition task.
  authors: Wenxiang Jiao, Michael R. Lyu and Irwin King
  link:
    url: https://www.aclweb.org/anthology/2020.findings-emnlp.435
    display: EMNLP 2020 (Findings)
  codelink:
    url: https://github.com/wxjiao/Pre-CODE
    display: Code
  highlight: 0
  
- title: "Real-Time Emotion Recognition via Attention Gated Hierarchical Memory Network"
  image: xxx.jpeg
  description: We propose an Attention Gated Hierarchical Memory Network (AGHMN) to build the memory bank carefully for capturing historical context and summarize the memories appropriately for real-time conversational emotion recognition.
  authors: Wenxiang Jiao, Michael R. Lyu and Irwin King
  link:
    url: https://aaai.org/ojs/index.php/AAAI/article/view/6309
    display: AAAI 2020
  codelink:
    url: https://github.com/wxjiao/AGHMN
    display: Code
  highlight: 0
  
- title: "HiGRU - Hierarchical Gated Recurrent Units for Utterance-Level Emotion Recognition"
  image: xxx.jpeg
  description: We propose a Hierarchical Gated Recurrent Unit (HiGRU) framework with a lower-level GRU to model the word-level inputs and an upper-level GRU to capture the contexts of utterance-level embeddings for conversational emotion recognition.
  authors: Wenxiang Jiao, Haiqin Yang, Irwin King and Michael R. Lyu
  link:
    url: https://www.aclweb.org/anthology/N19-1037
    display: NAACL 2019
  codelink:
    url: https://github.com/wxjiao/HiGRUs
    display: Code
  highlight: 0
  
